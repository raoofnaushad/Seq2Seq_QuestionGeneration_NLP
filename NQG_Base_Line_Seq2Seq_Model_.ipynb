{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NQG-Base Line Seq2Seq Model .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z4VpQQNeFPiz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VpQQNeFPiz",
        "colab_type": "text"
      },
      "source": [
        "# Mounting Drive for dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDYwChViCFv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b3a9b2d-b7ef-4693-9b95-bb660c45db11"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzCk2qnkF-vc",
        "colab_type": "text"
      },
      "source": [
        "# Importing Libraries, Config and Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhL-i-w5IV9N",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKfhk8H-FxNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import spacy\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-xfIHLVIYbP",
        "colab_type": "text"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPv6s3XkF88U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "493b4faa-88ab-4dd8-f03b-60f0b538ca1b"
      },
      "source": [
        "# !python -m spacy download en     ## If tokenization error try to update the spacyt package\n",
        "\n",
        "DATASET_BASE_PATH = '/content/drive/My Drive/csv_for_nqg'\n",
        "TRAIN_FILENAME =  'train-v2.csv'\n",
        "VALID_FILENAME = 'dev-v2.csv'\n",
        "train_df = pd.read_csv(os.path.join(DATASET_BASE_PATH, TRAIN_FILENAME))\n",
        "valid_df = pd.read_csv(os.path.join(DATASET_BASE_PATH, VALID_FILENAME))\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6f1c683c1a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTRAIN_FILENAME\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m'train-v2.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mVALID_FILENAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dev-v2.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_BASE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_BASE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALID_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_TuWxw0IZ1H",
        "colab_type": "text"
      },
      "source": [
        "## Setting seeds for deterministic results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxjiNMPoGKMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI_SWl0QJfYE",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OCI9Wf-GodM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text):\n",
        "  '''\n",
        "  Function for tokenizing with spacy english model\n",
        "  '''\n",
        "  return  [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAoVi8ocLeun",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibdH_dwhLn-E",
        "colab_type": "text"
      },
      "source": [
        "## Analysing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQcsDPYcLq0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "73be9a0e-1d0e-4680-da0f-8f362d54672b"
      },
      "source": [
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18222, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>sentence</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>sent_ans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Beyonce would take a break from music in which...</td>\n",
              "      <td>2010</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Which year did Beyonce and her father part bus...</td>\n",
              "      <td>2010</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Beyonc√© 's musical break lasted nine months an...</td>\n",
              "      <td>Which famous landmark did Beyonce see in China ?</td>\n",
              "      <td>the Great Wall of China</td>\n",
              "      <td>Beyonc√© 's musical break lasted nine months an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>In what year did Beyonce have her hiatus ?</td>\n",
              "      <td>2010</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "      <td>Who inspired this hiatus ?</td>\n",
              "      <td>her mother</td>\n",
              "      <td>Beyonc√© announced a hiatus from her music care...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...                                           sent_ans\n",
              "0  Beyonc√© announced a hiatus from her music care...  ...  Beyonc√© announced a hiatus from her music care...\n",
              "1  Beyonc√© announced a hiatus from her music care...  ...  Beyonc√© announced a hiatus from her music care...\n",
              "2  Beyonc√© announced a hiatus from her music care...  ...  Beyonc√© 's musical break lasted nine months an...\n",
              "3  Beyonc√© announced a hiatus from her music care...  ...  Beyonc√© announced a hiatus from her music care...\n",
              "4  Beyonc√© announced a hiatus from her music care...  ...  Beyonc√© announced a hiatus from her music care...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yki71gHFLhFr",
        "colab_type": "text"
      },
      "source": [
        "## Creating Fields for Data\n",
        "We only need one type of preprocessing for all required columns so we need one Field with\n",
        "* sequential = **True** Since our input is text and it is sequential\n",
        "* tokenize = We are tokenizing using a function called **tokenizer** which is build using a **spacy english langauge model**\n",
        "* init_token => Need to append initial token to every input\n",
        "* eos_token =>  Need to append end token to every input\n",
        "* lower => We need to lower every tokens as a preprocessing step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNeGfJcbKcz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIELD = Field(sequential=True,\n",
        "                   tokenize=tokenizer,\n",
        "                   init_token='<sos>',\n",
        "                   eos_token='<eos>',\n",
        "                   lower=True)\n",
        "\n",
        "TRAIN_VALID_FIELDS = [('context', None), ## For this phase of the project we are not using context, So no field assigned\n",
        "                    ('sentence', None), ## Sentence is one of the column of interest and preprocessing is needed so Field is also needed\n",
        "                    ('question', FIELD), ## Question is another columnd of interest\n",
        "                    ('answer', None), ## We will not be using answer at this phase\n",
        "                    ('sent_ans', FIELD) ## Not now\n",
        "                    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeJ8Ns1FPzmC",
        "colab_type": "text"
      },
      "source": [
        "## Creating Dataset from fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drh3snnYNlxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = TabularDataset.splits(path=DATASET_BASE_PATH,\n",
        "                                        format='csv',\n",
        "                                        train = TRAIN_FILENAME,\n",
        "                                        validation = VALID_FILENAME,\n",
        "                                        fields = TRAIN_VALID_FIELDS,\n",
        "                                        skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uDZlAdpTtIK",
        "colab_type": "text"
      },
      "source": [
        "## Checking data from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agmj4UkOSzmd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "2f61bb96-5966-4fc4-f9bc-7b2f0b0cd59a"
      },
      "source": [
        "print(f\"Type of train data => {type(train_data)} and valid data => {type(valid_data)}\")\n",
        "print(f\"Length of train data => {len(train_data)} and valid data => {len(valid_data)}\")\n",
        "print(train_data.fields.items())\n",
        "example = train_data[0]\n",
        "print(type(example))\n",
        "print(example.sent_ans, example.question)\n",
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of train data => <class 'torchtext.data.dataset.TabularDataset'> and valid data => <class 'torchtext.data.dataset.TabularDataset'>\n",
            "Length of train data => 18222 and valid data => 871\n",
            "dict_items([('context', None), ('sentence', None), ('question', <torchtext.data.field.Field object at 0x7fd1e9a19dd8>), ('answer', None), ('sent_ans', <torchtext.data.field.Field object at 0x7fd1e9a19dd8>)])\n",
            "<class 'torchtext.data.example.Example'>\n",
            "['beyonc√©', 'announced', 'a', 'hiatus', 'from', 'her', 'music', 'career', 'in', 'january', '2010', ',', 'heeding', 'her', 'mother', \"'s\", 'advice', ',', '\"', 'to', 'live', 'life', ',', 'to', 'be', 'inspired', 'by', 'things', 'again', '\"', '.', 'answer', '2010'] ['beyonce', 'would', 'take', 'a', 'break', 'from', 'music', 'in', 'which', 'year', '?']\n",
            "{'question': ['beyonce', 'would', 'take', 'a', 'break', 'from', 'music', 'in', 'which', 'year', '?'], 'sent_ans': ['beyonc√©', 'announced', 'a', 'hiatus', 'from', 'her', 'music', 'career', 'in', 'january', '2010', ',', 'heeding', 'her', 'mother', \"'s\", 'advice', ',', '\"', 'to', 'live', 'life', ',', 'to', 'be', 'inspired', 'by', 'things', 'again', '\"', '.', 'answer', '2010']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qb6AUy_DbSS",
        "colab_type": "text"
      },
      "source": [
        "## Building Vocabulary from training_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U32ML9F8S_oS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f362dde-4dbd-4fdc-bffb-3d5bcc1fe147"
      },
      "source": [
        "FIELD.build_vocab(train_data, valid_data, min_freq=2)\n",
        "\n",
        "print(f\"Total length of train vocabulary is {len(FIELD.vocab)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total length of train vocabulary is 24953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggqfRdJvIkpl",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Batches of Data using Bucket Iterator\n",
        "This also takes care of padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_0pDiRhGgHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.sent_ans),\n",
        "    device = DEVICE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI0VcxHLKILE",
        "colab_type": "text"
      },
      "source": [
        "## Understanding Iterator\n",
        "* Here we can see each iteration will give us a batch of text data.\n",
        "* How to access data in batch \n",
        "* Fields associated in each batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiWIbH3_KAqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "614dc301-db46-4fc6-f9f5-926c6cd6fbea"
      },
      "source": [
        "print(f\"Size of the train and valid iterators =>>> {len(train_iterator)}, {len(valid_iterator)}\")\n",
        "batch = next(iter(train_iterator))\n",
        "print(type(batch))\n",
        "print(batch.sent_ans, batch.sent_ans.shape)\n",
        "print(batch.question, batch.question.shape)\n",
        "print(batch.dataset.fields)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the train and valid iterators =>>> 143, 7\n",
            "<class 'torchtext.data.batch.Batch'>\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [  21,  172,   54,  ...,  247,  212,   41],\n",
            "        [3384,   45,    5,  ..., 1153,    5,   15],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0') torch.Size([89, 128])\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [   7,   13,   13,  ...,   35,  354,  256],\n",
            "        [3384,  565,   16,  ..., 2284,   40,  123],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0') torch.Size([21, 128])\n",
            "{'context': None, 'sentence': None, 'question': <torchtext.data.field.Field object at 0x7fd1e9a19dd8>, 'answer': None, 'sent_ans': <torchtext.data.field.Field object at 0x7fd1e9a19dd8>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtghXxMCMucH",
        "colab_type": "text"
      },
      "source": [
        "**NOTE**\n",
        "* You can see that BucketIterator returns a batch object instead of sentence tensors and question tensors\n",
        "* Also we can't iterate through batch object \n",
        "- We can overcome this problem by writing a wrapper around Iterator object which will return required data or we can write some extra code for the same. In this tutorial we will move with the second approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejLUtflKYArU",
        "colab_type": "text"
      },
      "source": [
        "# Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlH_e2VdOVvb",
        "colab_type": "text"
      },
      "source": [
        "### Let's see what all things we have done until now.\n",
        "### **Initial Steps**\n",
        "1. Downloaded the SQUAD dataset\n",
        "2. Preprocess the dataset and converted into a csv with the following coloumns\n",
        "  - context \n",
        "  - sentence\n",
        "  - question\n",
        "  - answer  \n",
        "3. Uploaded train and test csv files to google drive.\n",
        "4. Mounted Drive in colab such that we don't have to upload data again in each session.\n",
        "### **Preparing Data**\n",
        "5. Installed spacy and english model of spacy for tokenization.\n",
        "6. Created **Fields** for train and test data. Since both needed same type of preprocessing and steps we used the same field for both.\n",
        "7. Created the dataset from CSV using **TabularDataset** a torchtext library. Which will help handling the data and also gives the preprocessed data as output with given field as argument.\n",
        "8. Created DataIterators for batching the data with torchtext library called BuckerIterator. This also taken care of the padding. Since it works on minibatching we also specified a sorting parameter so that in each batch the iterator batches datas with similar length.\n",
        "\n",
        "**Note** : The first points are not mentioned in the notebook. Kindly check my github repo for it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmPA4mjdgdN8",
        "colab_type": "text"
      },
      "source": [
        "# Building Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VygLWnYgbkH",
        "colab_type": "text"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "In the forward method, we pass in the source sentence,  ùëã , which is converted into dense vectors using the embedding layer, and then dropout is applied. These embeddings are then passed into the RNN. As we pass a whole sequence to the RNN, it will automatically do the recurrent calculation of the hidden states over the whole sequence for us! Notice that we do not pass an initial hidden or cell state to the RNN. This is because, as noted in the documentation, that if no hidden/cell state is passed to the RNN, it will automatically create an initial hidden/cell state as a tensor of all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U57UZ8tzgbr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUqIYb6lgbDh",
        "colab_type": "text"
      },
      "source": [
        "## Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNxXQSnSgbPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhEG6m_rgacZ",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k93cGUXGganq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEq0wh0oglww",
        "colab_type": "text"
      },
      "source": [
        "# Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTQ7G0zgG3i",
        "colab_type": "text"
      },
      "source": [
        "## Initialize model and its parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXfD0gQrgHGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(FIELD.vocab)\n",
        "OUTPUT_DIM = len(FIELD.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Kz7KdjmeIT",
        "colab_type": "text"
      },
      "source": [
        "### Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OSA0REFme3F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "1a793521-1bbe-42b9-961d-cbc3621be1b6"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.5, 0.5)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(24953, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(24953, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=24953, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx0-31zTmfDO",
        "colab_type": "text"
      },
      "source": [
        "### Counting trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QTxK6ZRmfMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29b9b104-dceb-4cd0-90bb-3f8519ef6065"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 32,933,241 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN3kpJn2cNDB",
        "colab_type": "text"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr8bR4ircNOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i, batch in enumerate(iterator):\n",
        "    sentence = batch.sent_ans\n",
        "    question = batch.question\n",
        "\n",
        "    ## Zeroing optimizer\n",
        "    optimizer.zero_grad()\n",
        "    ## One pass to model with single batch\n",
        "    outputs = model(sentence, question)\n",
        "    output_dim = outputs.shape[-1]\n",
        "    output = outputs[1:].view(-1, output_dim)\n",
        "    question = question[1:].view(-1)\n",
        "    ## finding loss\n",
        "    loss = criterion(output, question)\n",
        "    ## Finding gradients\n",
        "    loss.backward()\n",
        "    ## Updating weights\n",
        "    optimizer.step()\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss/len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuU1uKGUcKI1",
        "colab_type": "text"
      },
      "source": [
        "## Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikwhm4PpcKVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      sentence = batch.sent_ans\n",
        "      question = batch.question\n",
        "      outputs = model(sentence, question)\n",
        "      output_dim = outputs.shape[-1]\n",
        "      output = outputs[1:].view(-1, output_dim)\n",
        "      question = question[1:].view(-1)\n",
        "      loss = criterion(output, question)\n",
        "      epoch_loss += loss\n",
        "  return epoch_loss/len(iterator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsMdDQfpcXeX",
        "colab_type": "text"
      },
      "source": [
        "## Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYZw0bkLcXr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "PAD_IDX = FIELD.vocab.stoi[FIELD.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIERp6a9bbNY",
        "colab_type": "text"
      },
      "source": [
        "## Function for time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PbMGSAZbbjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elapsed_time(start_time, end_time):\n",
        "  total_time = end_time - start_time\n",
        "  epoch_mins = int((total_time)/60)\n",
        "  epoch_secs = int(total_time - (epoch_mins*60))\n",
        "  return epoch_mins, epoch_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOAPQGoPYQ1z",
        "colab_type": "text"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCHE7gHYFw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04d7990d-9cd4-4b17-c3d5-b4c6f44f5f7f"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "BEST_VALID_LOSS = np.Inf\n",
        "MODEL_DIR = os.path.join(DATASET_BASE_PATH, 'model.pt')\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "  valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "  if valid_loss < BEST_VALID_LOSS:\n",
        "    BEST_VALID_LOSS = valid_loss\n",
        "    torch.save(model.state_dict(), MODEL_DIR)\n",
        "  end_time = time.time()\n",
        "  epoch_min, epoch_sec = elapsed_time(start_time, end_time)\n",
        "  print(f\"Epoch:{epoch}\")\n",
        "  print(f\"TIME: {epoch_min} minutes : {epoch_sec} seconds\")\n",
        "  print(f\"Training loss {train_loss:.3f} ||  Perplexity {math.exp(train_loss):7.3f}\")\n",
        "  print(f\"Validation loss {valid_loss:.3f} ||  Perplexity {math.exp(valid_loss):7.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:0\n",
            "TIME: 0 minutes : 53 seconds\n",
            "Training loss 10.191 ||  Perplexity 26649.339\n",
            "Validation loss 8.235 ||  Perplexity 3770.117\n",
            "Epoch:1\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 7.110 ||  Perplexity 1224.240\n",
            "Validation loss 6.927 ||  Perplexity 1019.037\n",
            "Epoch:2\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 6.392 ||  Perplexity 597.259\n",
            "Validation loss 6.670 ||  Perplexity 788.066\n",
            "Epoch:3\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 6.130 ||  Perplexity 459.511\n",
            "Validation loss 6.553 ||  Perplexity 701.328\n",
            "Epoch:4\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 6.005 ||  Perplexity 405.520\n",
            "Validation loss 6.493 ||  Perplexity 660.677\n",
            "Epoch:5\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.936 ||  Perplexity 378.295\n",
            "Validation loss 6.491 ||  Perplexity 659.335\n",
            "Epoch:6\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.890 ||  Perplexity 361.346\n",
            "Validation loss 6.470 ||  Perplexity 645.485\n",
            "Epoch:7\n",
            "TIME: 0 minutes : 55 seconds\n",
            "Training loss 5.853 ||  Perplexity 348.344\n",
            "Validation loss 6.449 ||  Perplexity 631.943\n",
            "Epoch:8\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.829 ||  Perplexity 340.151\n",
            "Validation loss 6.460 ||  Perplexity 638.772\n",
            "Epoch:9\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.806 ||  Perplexity 332.375\n",
            "Validation loss 6.437 ||  Perplexity 624.224\n",
            "Epoch:10\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.783 ||  Perplexity 324.780\n",
            "Validation loss 6.438 ||  Perplexity 625.405\n",
            "Epoch:11\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.769 ||  Perplexity 320.215\n",
            "Validation loss 6.434 ||  Perplexity 622.933\n",
            "Epoch:12\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.754 ||  Perplexity 315.402\n",
            "Validation loss 6.452 ||  Perplexity 634.253\n",
            "Epoch:13\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.739 ||  Perplexity 310.654\n",
            "Validation loss 6.443 ||  Perplexity 628.342\n",
            "Epoch:14\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.727 ||  Perplexity 306.901\n",
            "Validation loss 6.431 ||  Perplexity 620.698\n",
            "Epoch:15\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.715 ||  Perplexity 303.296\n",
            "Validation loss 6.442 ||  Perplexity 627.941\n",
            "Epoch:16\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.702 ||  Perplexity 299.605\n",
            "Validation loss 6.444 ||  Perplexity 628.792\n",
            "Epoch:17\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.689 ||  Perplexity 295.643\n",
            "Validation loss 6.442 ||  Perplexity 627.854\n",
            "Epoch:18\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.682 ||  Perplexity 293.660\n",
            "Validation loss 6.466 ||  Perplexity 643.223\n",
            "Epoch:19\n",
            "TIME: 0 minutes : 54 seconds\n",
            "Training loss 5.669 ||  Perplexity 289.793\n",
            "Validation loss 6.460 ||  Perplexity 639.162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqcAvDoPIMW4",
        "colab_type": "text"
      },
      "source": [
        "## Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6RibtN7bHCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "12ecd4ca-268d-4dab-9620-4ffc87437e9c"
      },
      "source": [
        "MODEL_DIR = os.path.join(DATASET_BASE_PATH, 'model.pt')\n",
        "model.load_state_dict(torch.load(MODEL_DIR))\n",
        "\n",
        "test_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 6.428 | Test PPL: 619.131 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}